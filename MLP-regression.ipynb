{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7391774,"sourceType":"datasetVersion","datasetId":4268747},{"sourceId":7395399,"sourceType":"datasetVersion","datasetId":4269923}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom math import cos, pi, floor, sin\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-15T18:09:25.233423Z","iopub.execute_input":"2024-01-15T18:09:25.233903Z","iopub.status.idle":"2024-01-15T18:09:25.241581Z","shell.execute_reply.started":"2024-01-15T18:09:25.233871Z","shell.execute_reply":"2024-01-15T18:09:25.240310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_json(path):\n    with open(path, 'r') as file:\n        data = json.load(file)\n    return data\n\ntrain_data = load_json(\"/kaggle/input/company-revenue/train_V3.jsonl\")\nvalidation_data = load_json(\"/kaggle/input/company-revenue/validation_V3.jsonl\")\ntest_data = load_json(\"/kaggle/input/company-revenue/test_V3.jsonl\")\nlabel = 1\n\nlabels = load_json(\"/kaggle/input/company-revenue/two_labels_clustering_result.json\")\nlabels = [key for key in labels.keys() if labels[key][\"label\"] == label]\ntrain_data = [sample for sample in train_data if sample[\"soleadify_id\"] in labels]\nvalidation_data = [sample for sample in validation_data if sample[\"soleadify_id\"] in labels]\ntest_data = [sample for sample in test_data if sample[\"soleadify_id\"] in labels]","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:09:25.244001Z","iopub.execute_input":"2024-01-15T18:09:25.245141Z","iopub.status.idle":"2024-01-15T18:11:02.072182Z","shell.execute_reply.started":"2024-01-15T18:09:25.245098Z","shell.execute_reply":"2024-01-15T18:11:02.070906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keys = list(train_data[0].keys())[1:]\n\ndef flatten(lst):\n    result = []\n    for item in lst:\n        if isinstance(item, list):\n            result.extend(flatten(item))\n        else:\n            result.append(item)\n    return result\n\ndef make_tensors(data):\n    return torch.tensor([flatten([sample[key] for key in keys]) for sample in data])\n\ntrain_tensors = make_tensors(train_data)\nvalidation_tensors = make_tensors(validation_data)\ntest_tensors = make_tensors(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:02.077945Z","iopub.execute_input":"2024-01-15T18:11:02.078978Z","iopub.status.idle":"2024-01-15T18:11:17.032404Z","shell.execute_reply.started":"2024-01-15T18:11:02.078934Z","shell.execute_reply":"2024-01-15T18:11:17.031097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# I have saved my API token with \"wandb_api\" as Label. \n# If you use some other Label make sure to change the same below. \nwandb_api = user_secrets.get_secret(\"wandb_api\") \n\nwandb.login(key=wandb_api)\n\n# Hyperparameters\nwandb.init(entity=\"mihail-chirobocea\", project=\"eda\", name=\"test_label1.5_plot\")\nwandb.config.batch_size = 64\nwandb.config.epochs = 14\n# wandb.config.learning_rate = 1e-6\nwandb.config.learning_rate = 2e-7\nwandb.config.bottleneck_dim = 256","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:17.034932Z","iopub.execute_input":"2024-01-15T18:11:17.036261Z","iopub.status.idle":"2024-01-15T18:11:49.418053Z","shell.execute_reply.started":"2024-01-15T18:11:17.036218Z","shell.execute_reply":"2024-01-15T18:11:49.417163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CycleAnnealScheduler:\n    def __init__(\n        self, optimizer, lr_max, lr_divider, cut_point, step_size, momentum=None\n    ):\n        self.lr_max = lr_max\n        self.lr_divider = lr_divider\n        self.cut_point = step_size // cut_point\n        self.step_size = step_size\n        self.iteration = 0\n        self.cycle_step = int(step_size * (1 - cut_point / 100) / 2)\n        self.momentum = momentum\n        self.optimizer = optimizer\n\n    def get_lr(self):\n        if self.iteration > 2 * self.cycle_step:\n            cut = (self.iteration - 2 * self.cycle_step) / (\n                self.step_size - 2 * self.cycle_step\n            )\n            lr = self.lr_max * (1 + (cut * (1 - 100) / 100)) / self.lr_divider\n\n        elif self.iteration > self.cycle_step:\n            cut = 1 - (self.iteration - self.cycle_step) / self.cycle_step\n            lr = self.lr_max * (1 + cut * (self.lr_divider - 1)) / self.lr_divider\n\n        else:\n            cut = self.iteration / self.cycle_step\n            lr = self.lr_max * (1 + cut * (self.lr_divider - 1)) / self.lr_divider\n\n        return lr\n\n    def get_momentum(self):\n        if self.iteration > 2 * self.cycle_step:\n            momentum = self.momentum[0]\n\n        elif self.iteration > self.cycle_step:\n            cut = 1 - (self.iteration - self.cycle_step) / self.cycle_step\n            momentum = self.momentum[0] + cut * (self.momentum[1] - self.momentum[0])\n\n        else:\n            cut = self.iteration / self.cycle_step\n            momentum = self.momentum[0] + cut * (self.momentum[1] - self.momentum[0])\n\n        return momentum\n\n    def step(self):\n        lr = self.get_lr()\n\n        if self.momentum is not None:\n            momentum = self.get_momentum()\n\n        self.iteration += 1\n\n        if self.iteration == self.step_size:\n            self.iteration = 0\n\n        for group in self.optimizer.param_groups:\n            group['lr'] = lr\n\n            if self.momentum is not None:\n                group['betas'] = (momentum, group['betas'][1])\n\n        return lr\n\n\ndef anneal_linear(start, end, proportion):\n    return start + proportion * (end - start)\n\n\ndef anneal_cos(start, end, proportion):\n    cos_val = cos(pi * proportion) + 1\n\n    return end + (start - end) / 2 * cos_val\n\n\nclass Phase:\n    def __init__(self, start, end, n_iter, anneal_fn):\n        self.start, self.end = start, end\n        self.n_iter = n_iter\n        self.anneal_fn = anneal_fn\n        self.n = 0\n\n    def step(self):\n        self.n += 1\n\n        return self.anneal_fn(self.start, self.end, self.n / self.n_iter)\n\n    def reset(self):\n        self.n = 0\n\n    @property\n    def is_done(self):\n        return self.n >= self.n_iter\n\n\nclass CycleScheduler:\n    def __init__(\n        self,\n        optimizer,\n        lr_max,\n        n_iter,\n        momentum=(0.95, 0.85),\n        divider=25,\n        warmup_proportion=0.3,\n        phase=('linear', 'cos'),\n    ):\n        self.optimizer = optimizer\n\n        phase1 = int(n_iter * warmup_proportion)\n        phase2 = n_iter - phase1\n        lr_min = lr_max / divider\n\n        phase_map = {'linear': anneal_linear, 'cos': anneal_cos}\n\n        self.lr_phase = [\n            Phase(lr_min, lr_max, phase1, phase_map[phase[0]]),\n            Phase(lr_max, lr_min / 1e4, phase2, phase_map[phase[1]]),\n        ]\n\n        self.momentum = momentum\n\n        if momentum is not None:\n            mom1, mom2 = momentum\n            self.momentum_phase = [\n                Phase(mom1, mom2, phase1, phase_map[phase[0]]),\n                Phase(mom2, mom1, phase2, phase_map[phase[1]]),\n            ]\n\n        else:\n            self.momentum_phase = []\n\n        self.phase = 0\n\n    def step(self):\n        lr = self.lr_phase[self.phase].step()\n\n        if self.momentum is not None:\n            momentum = self.momentum_phase[self.phase].step()\n\n        else:\n            momentum = None\n\n        for group in self.optimizer.param_groups:\n            group['lr'] = lr\n\n            if self.momentum is not None:\n                if 'betas' in group:\n                    group['betas'] = (momentum, group['betas'][1])\n\n                else:\n                    group['momentum'] = momentum\n\n        if self.lr_phase[self.phase].is_done:\n            self.phase += 1\n\n        if self.phase >= len(self.lr_phase):\n            for phase in self.lr_phase:\n                phase.reset()\n\n            for phase in self.momentum_phase:\n                phase.reset()\n\n            self.phase = 0\n\n        return lr, momentum","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:49.428388Z","iopub.execute_input":"2024-01-15T18:11:49.428848Z","iopub.status.idle":"2024-01-15T18:11:49.465630Z","shell.execute_reply.started":"2024-01-15T18:11:49.428811Z","shell.execute_reply":"2024-01-15T18:11:49.464170Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\ntrain_dataset = MyDataset(train_tensors)\nvalidation_dataset = MyDataset(validation_tensors)\ntest_dataset = MyDataset(test_tensors)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=wandb.config.batch_size, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=wandb.config.batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:49.468684Z","iopub.execute_input":"2024-01-15T18:11:49.469255Z","iopub.status.idle":"2024-01-15T18:11:49.511565Z","shell.execute_reply.started":"2024-01-15T18:11:49.469209Z","shell.execute_reply":"2024-01-15T18:11:49.510161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QuantizationLayer(nn.Module):\n    def __init__(self, num_clusters, latent_dim):\n        super(QuantizationLayer, self).__init__()\n        self.num_clusters = num_clusters\n        self.codebook = nn.Parameter(torch.rand(num_clusters, latent_dim))\n\n    def forward(self, x):\n        # Calculate distances between inputs and codebook vectors\n        distances = torch.norm(x.detach().unsqueeze(1) - self.codebook, dim=-1)\n\n        # Find the index of the closest codebook vector for each input\n        quantized_indices = torch.argmin(distances, dim=-1)\n\n        # Retrieve the corresponding codebook vectors\n        quantized_inputs = self.codebook[quantized_indices]\n\n        return quantized_inputs, quantized_indices\n\nclass FCBlock(nn.Module):\n    def __init__(self, in_dim, out_dim, use_norm=True, activation=True):\n        super(FCBlock, self).__init__()\n        self.fc = nn.Linear(in_dim, out_dim)\n        self.use_norm = use_norm\n        if activation:\n            self.activation = nn.SiLU()\n        else: \n            self.activation = False\n        if use_norm:\n            self.norm = nn.LayerNorm(out_dim)\n        else:\n            self.norm = False\n\n    def forward(self, x):\n        x = self.fc(x)\n        if self.use_norm:\n            x = self.norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x\n\n    \nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        in_dim,\n        embed_dim,\n    ):\n        super().__init__()\n        \n        self.encoder = nn.Sequential(\n            FCBlock(input_dim, 8 * embed_dim),\n#             FCBlock(8 * embed_dim, 8 * embed_dim),\n            FCBlock(8 * embed_dim, 4 * embed_dim),\n#             FCBlock(4 * embed_dim, 4 * embed_dim),\n            FCBlock(4 * embed_dim, 2 * embed_dim),\n#             FCBlock(2 * embed_dim, 2 * embed_dim),\n            FCBlock(2 * embed_dim, embed_dim),\n#             FCBlock(embed_dim, embed_dim),\n#             FCBlock(embed_dim, embed_dim),\n            FCBlock(embed_dim, embed_dim),\n            FCBlock(embed_dim, embed_dim),\n            FCBlock(embed_dim, embed_dim),\n        ) \n            \n    def forward(self, x):\n        return self.encoder(x)\n    \n    \nclass VQVAE(nn.Module):\n    def __init__(\n        self,\n        in_dim,\n        embed_dim,\n        num_clusters=4096,\n    ):\n        super().__init__()\n        self.latent_dim = embed_dim // 2\n\n        self.encoder = Encoder(in_dim, embed_dim)\n        \n        self.decoder = nn.Sequential(\n            FCBlock(self.latent_dim, embed_dim),\n            FCBlock(embed_dim, embed_dim),\n            FCBlock(embed_dim, embed_dim),\n            FCBlock(embed_dim, embed_dim),\n            FCBlock(embed_dim, embed_dim),\n            FCBlock(embed_dim, embed_dim),\n            FCBlock(embed_dim, 2 * embed_dim),\n            FCBlock(2 * embed_dim, 2 * embed_dim),\n            FCBlock(2 * embed_dim, 4 * embed_dim),\n            FCBlock(4 * embed_dim, 4 * embed_dim),\n            FCBlock(4 * embed_dim, 8 * embed_dim),\n            FCBlock(8 * embed_dim, 8 * embed_dim),\n            FCBlock(8 * embed_dim, in_dim, activation=False)\n        )\n\n        # Quantization layer\n        self.quantization = QuantizationLayer(num_clusters, self.latent_dim)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def encode(self, x):\n        return self.encoder(x)\n    \n    def forward(self, x):\n        new_x = x.clone()\n        new_x[:, 7] = torch.zeros(new_x.shape[0])\n        # Encode\n        enc_output = self.encoder(new_x)\n        mu, logvar = enc_output[:, :self.latent_dim], enc_output[:, self.latent_dim:]\n\n        # Reparameterize\n        z = self.reparameterize(mu, logvar)\n\n        # Quantize the latent space\n        quantized_z, quantized_indices = self.quantization(z)\n\n        # Decode\n        x_recon = self.decoder(quantized_z)\n\n        return x_recon, mu, logvar, quantized_indices","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:49.513733Z","iopub.execute_input":"2024-01-15T18:11:49.514383Z","iopub.status.idle":"2024-01-15T18:11:49.541342Z","shell.execute_reply.started":"2024-01-15T18:11:49.514349Z","shell.execute_reply":"2024-01-15T18:11:49.540203Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self, input_dim, embed_dim):\n        super(Autoencoder, self).__init__()\n\n        self.encoder = Encoder(input_dim, embed_dim)\n        self.decoder = nn.Sequential(\n            FCBlock(embed_dim, 2 * embed_dim),\n            FCBlock(2 * embed_dim, 2 * embed_dim),\n            FCBlock(2 * embed_dim, 4 * embed_dim),\n            FCBlock(4 * embed_dim, 4 * embed_dim),\n            FCBlock(4 * embed_dim, 8 * embed_dim),\n            FCBlock(8 * embed_dim, 8 * embed_dim),\n            FCBlock(8 * embed_dim, input_dim, activation=False)\n        )\n\n    def forward(self, x):\n        new_x = x.clone()\n        new_x[:, 7] = torch.zeros(new_x.shape[0])\n        encoded = self.encoder(new_x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \nclass Predictor(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(Predictor, self).__init__()\n\n        self.predict = nn.Sequential(\n            FCBlock(input_dim, input_dim // 4),\n            FCBlock(input_dim // 4, input_dim // 8),\n            FCBlock(input_dim // 8, output_dim),\n        ) \n\n    def forward(self, x):\n        return self.predict(x)\n    \nclass FullModel(nn.Module):\n    def __init__(self, input_dim, output_dim, encoder):\n        super(FullModel, self).__init__()\n        \n        self.encoder = encoder\n        self.predict = Predictor(input_dim, output_dim)\n\n    def forward(self, x):\n#         print(f\"Original revenue: {x[0][7]} denormalized: {x[0][7]* 789799993000 + 7000}\")\n        new_x = x.clone()\n        new_x[:, 7] = torch.zeros(new_x.shape[0])\n#         print(f\"Seen revenue: {x[0][7]}\")\n        new_x = self.encoder(new_x)\n        new_x = self.predict(new_x)\n#         print(f\"Predicted revenue: {x[0].item()} denormalized: {x[0].item()* 789799993000 + 7000}\")\n        return new_x","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:49.543498Z","iopub.execute_input":"2024-01-15T18:11:49.544069Z","iopub.status.idle":"2024-01-15T18:11:49.563927Z","shell.execute_reply.started":"2024-01-15T18:11:49.544022Z","shell.execute_reply":"2024-01-15T18:11:49.562883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_vqautoencoder(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, num_epochs=10, device=\"cpu\", save_path=\"best_model.pth\"):\n#     model.to(device)\n#     best_val_loss = float('inf')\n    \n#     for epoch in range(num_epochs):\n#         model.train()\n#         running_loss = 0.0\n        \n#         for k, batch_data in enumerate(train_dataloader):\n            \n#             optimizer.zero_grad()\n#             inputs = batch_data.to(device)\n            \n#             # Forward pass\n#             recon_batch, mu, logvar, quantized_indices = model(inputs)\n\n#             # Compute reconstruction loss and KL divergence\n#             reconstruction_loss = criterion(recon_batch, batch_data)\n#             kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n#             # Quantization loss - encourage the codebook to adapt to the data\n#             quantization_loss = torch.mean(torch.norm(mu - model.quantization.codebook[quantized_indices], dim=-1))\n\n#             # Total loss\n#             loss = reconstruction_loss + kl_divergence + quantization_loss\n#             running_loss += reconstruction_loss.item()\n            \n#             # Backward pass and optimization\n#             loss.backward()\n#             scheduler.step()\n#             optimizer.step()\n            \n#             if k % 100 == 0:\n#                 print(f\"Training - Epoch [{epoch + 1}/{num_epochs}], Step [{k}], Loss: {reconstruction_loss.item():.4f}\")\n#                 wandb.log({\"Train Loss\": reconstruction_loss.item()})\n#                 wandb.log({\"Learning Rate\": optimizer.param_groups[0]['lr']})\n                \n#         # Calculate average training loss for the epoch\n#         average_train_loss = running_loss / len(train_dataloader)\n#         print(f\"Training - Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_train_loss:.4f}\")\n#         wandb.log({\"Train Epoch Loss\": average_train_loss, \"Epoch\": epoch})\n    \n#         # Validation\n#         model.eval()\n#         val_running_loss = 0.0\n#         with torch.no_grad():\n#             for k, val_data in enumerate(val_dataloader):\n\n#                 inputs = val_data.to(device)\n\n#                 # Forward pass\n#                 recon_batch, mu, logvar, quantized_indices = model(inputs)\n\n#                 # Compute reconstruction loss and KL divergence\n#                 reconstruction_loss = criterion(recon_batch, val_data)\n#                 kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n#                 # Quantization loss - encourage the codebook to adapt to the data\n#                 quantization_loss = torch.mean(torch.norm(mu - model.quantization.codebook[quantized_indices], dim=-1))\n\n#                 # Total loss\n#                 val_loss = reconstruction_loss + kl_divergence + quantization_loss\n#                 val_running_loss += reconstruction_loss.item()\n                \n#         # Calculate average validation loss for the epoch\n#         average_val_loss = val_running_loss / len(val_dataloader)\n#         print(f\"Validation - Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_val_loss:.8f}\")\n#         wandb.log({\"Validation Epoch Loss\": average_val_loss, \"Epoch\": epoch})\n\n#         # Save the best model\n#         if average_val_loss < best_val_loss:\n#             best_val_loss = average_val_loss\n#             torch.save(model.state_dict(), save_path)\n#             print(f\"Best model saved with validation loss: {best_val_loss:.4f} at epoch {epoch + 1}\")\n\n#     print(\"Training complete.\")\n#     wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:49.565622Z","iopub.execute_input":"2024-01-15T18:11:49.567423Z","iopub.status.idle":"2024-01-15T18:11:49.580628Z","shell.execute_reply.started":"2024-01-15T18:11:49.567374Z","shell.execute_reply":"2024-01-15T18:11:49.579544Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_autoencoder(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, num_epochs=10, device=\"cpu\", save_path=\"best_model.pth\"):\n    model.to(device)\n    best_val_loss = float('inf')\n#     std = 0.006515\n    all_preds = []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        running_metric = 0.0\n        \n#         for k, batch_data in enumerate(train_dataloader):\n            \n#             optimizer.zero_grad()\n#             inputs = batch_data.to(device)\n            \n#             # Forward pass\n#             recon_batch = model(inputs).flatten().clamp(0, 1)\n#             reconstruction_loss = criterion((recon_batch * (789800000000-7000) + 7000), batch_data[:, 7] * (789800000000-7000) + 7000)\n#             metric = F.l1_loss((recon_batch * (789800000000-7000) + 7000), batch_data[:, 7] * (789800000000-7000) + 7000)\n            \n#             if epoch > -1:\n#                 if k % 100 == 0:\n#                     print(f\"Original revenue: {batch_data[0][7]}\")\n#                     print(f\"Predicted revenue: {recon_batch[0]}\")\n#                     print(f\"Error: {abs((batch_data[0][7]) - (recon_batch[0]))}\")\n\n#             # Total loss\n#             loss = reconstruction_loss\n#             running_loss += reconstruction_loss.item()\n#             running_metric += metric.item()\n            \n#             # Backward pass and optimization\n#             loss.backward()\n#             scheduler.step()\n#             optimizer.step()\n            \n#             if k % 100 == 0:\n#                 print(f\"Training - Epoch [{epoch + 1}/{num_epochs}], Step [{k}], Loss: {reconstruction_loss.item():.4f}\")\n#                 wandb.log({\"Train MSE Loss\": reconstruction_loss.item()})\n#                 wandb.log({\"Train L1 Metric\": metric.item()})\n#                 wandb.log({\"Learning Rate\": optimizer.param_groups[0]['lr']})\n                \n#         # Calculate average training loss for the epoch\n#         average_train_loss = running_loss / len(train_dataloader)\n#         average_train_metric = running_metric / len(train_dataloader)\n#         print(f\"Training - Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_train_loss:.4f}\")\n#         wandb.log({\"Train Epoch Loss\": average_train_loss, \"Epoch\": epoch})\n#         wandb.log({\"Train Epoch Metric\": average_train_metric, \"Epoch\": epoch})\n    \n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        with torch.no_grad():\n            for k, val_data in enumerate(val_dataloader):\n\n                inputs = val_data.to(device)\n\n                # Forward pass\n                recon_batch = model(inputs).flatten().clamp(0, 1)\n                predicted = recon_batch.detach().cpu().numpy()\n                all_preds.append(predicted)\n#                 reconstruction_loss = criterion((recon_batch * 789800000000 + 7000), val_data[:, 7] * 789800000000 + 7000)\n                metric = F.l1_loss((recon_batch * (789800000000-7000) + 7000), val_data[:, 7] * (789800000000-7000) + 7000)\n                val_running_loss += metric.item()\n                \n        # Calculate average validation loss for the epoch\n        average_val_loss = val_running_loss / len(val_dataloader)\n        print(f\"Validation - Epoch [{epoch + 1}/{num_epochs}], Average Metric: {average_val_loss:.8f}\")\n        wandb.log({\"Validation Epoch Metric\": average_val_loss, \"Epoch\": epoch})\n\n        # Save the best model\n        if average_val_loss < best_val_loss:\n            best_val_loss = average_val_loss\n            torch.save(model.state_dict(), save_path)\n            print(f\"Best model saved with validation loss: {best_val_loss:.4f} at epoch {epoch + 1}\")\n\n    print(\"Training complete.\")\n    wandb.finish()\n    return all_preds","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:49.583230Z","iopub.execute_input":"2024-01-15T18:11:49.584113Z","iopub.status.idle":"2024-01-15T18:11:49.604370Z","shell.execute_reply.started":"2024-01-15T18:11:49.584067Z","shell.execute_reply":"2024-01-15T18:11:49.602897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from torch.autograd import Variable\n\n# def validate_autoencoder(model, val_dataloader, criterion, device=\"cpu\"):\n#     model.eval()\n#     val_running_loss = 0.0\n#     all_gradients = []  # List to store gradients for each batch\n    \n#     for k, val_data in enumerate(val_dataloader):\n#         inputs = val_data.to(device)\n\n#         # Forward pass\n#         recon_batch = model(inputs)\n\n#         reconstruction_loss = criterion(recon_batch, val_data[7])\n#         val_running_loss += reconstruction_loss.item()\n\n#         # Compute gradients for input features\n#         input_data_tensor = Variable(val_data, requires_grad=True).to(device)\n#         gradients = input_gradients(model, input_data_tensor)\n#         all_gradients.append(gradients.numpy())\n\n#     # Calculate average validation loss\n#     average_val_loss = val_running_loss / len(val_dataloader)\n#     print(f\"Average Validation Loss: {average_val_loss:.8f}\")\n\n#     # Calculate average gradients over all batches\n#     avg_gradients = np.mean(all_gradients, axis=0)\n\n#     # Plot the average gradients\n#     plot_gradients(avg_gradients)\n\n# def plot_gradients(avg_gradients):\n#     num_features = avg_gradients.shape[1]\n#     feature_names = [f\"Feature {i}\" for i in range(1, num_features + 1)]\n\n#     plt.bar(feature_names, avg_gradients)\n#     plt.xlabel('Input Features')\n#     plt.ylabel('Average Gradients')\n#     plt.title('Average Gradients for Input Features')\n#     plt.show()\n\n# def input_gradients(model, input_data, target_index=0):\n#     input_data = Variable(input_data, requires_grad=True)\n#     model.eval()  # Set the model to evaluation mode\n\n#     output = model(input_data)\n    \n#     # Choose a specific element or aggregation of elements from the output for gradient computation\n#     target_value = output[0, target_index]  # You may need to adjust this based on your model's output shape\n\n#     target_value.backward()\n#     gradients = input_data.grad.data\n\n#     # You can return gradients as a numpy array if needed\n#     return gradients","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:49.611470Z","iopub.execute_input":"2024-01-15T18:11:49.612907Z","iopub.status.idle":"2024-01-15T18:11:49.624438Z","shell.execute_reply.started":"2024-01-15T18:11:49.612849Z","shell.execute_reply":"2024-01-15T18:11:49.623250Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dim = 1740\nautoencoder = Autoencoder(input_dim, wandb.config.bottleneck_dim)\n# autoencoder.load_state_dict(torch.load(\"/kaggle/input/models-eda/autoencoder_v16.pth\"))\nautoencoder = FullModel(256, 1, autoencoder.encoder)\nautoencoder.load_state_dict(torch.load(\"/kaggle/input/models-eda/N-L1_label_1_autoencoder_V0.5.pth\"))\n# Define training parameters\ncriterion = nn.MSELoss()\noptimizer = AdamW(autoencoder.parameters(), lr=wandb.config.learning_rate)\nscheduler = CycleScheduler(\n    optimizer,\n    wandb.config.learning_rate,\n    n_iter=len(train_dataloader) * wandb.config.epochs,\n    momentum=None,\n    warmup_proportion=0.15,\n)\n\n# Train the autoencoder\ny_val_pred = train_autoencoder(autoencoder, train_dataloader, test_dataloader, criterion, optimizer, scheduler, num_epochs=wandb.config.epochs, save_path=\"N-L1_label_1_autoencoder_V0.5.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:11:49.627085Z","iopub.execute_input":"2024-01-15T18:11:49.628204Z","iopub.status.idle":"2024-01-15T18:12:16.205546Z","shell.execute_reply.started":"2024-01-15T18:11:49.628152Z","shell.execute_reply":"2024-01-15T18:12:16.204206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:12:16.208672Z","iopub.execute_input":"2024-01-15T18:12:16.209734Z","iopub.status.idle":"2024-01-15T18:12:16.218552Z","shell.execute_reply.started":"2024-01-15T18:12:16.209663Z","shell.execute_reply":"2024-01-15T18:12:16.217310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_pred_f = []\nfor sample in y_val_pred:\n    y_val_pred_f.extend(sample)\nnp.array(y_val_pred_f[:3546]).shape","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:12:16.220577Z","iopub.execute_input":"2024-01-15T18:12:16.221002Z","iopub.status.idle":"2024-01-15T18:12:16.242610Z","shell.execute_reply.started":"2024-01-15T18:12:16.220971Z","shell.execute_reply":"2024-01-15T18:12:16.241402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val = [sample[\"estimated_revenue\"] for sample in test_data]\nnp.array(y_val).shape","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:12:16.244902Z","iopub.execute_input":"2024-01-15T18:12:16.245281Z","iopub.status.idle":"2024-01-15T18:12:16.258705Z","shell.execute_reply.started":"2024-01-15T18:12:16.245249Z","shell.execute_reply":"2024-01-15T18:12:16.257338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.kdeplot(y_val, label='True Values', fill=True)\nsns.kdeplot(y_val_pred_f, label='Predicted Values', fill=True)\nplt.title('True vs Predicted Values on Test - Label 1 ')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:13:56.496096Z","iopub.execute_input":"2024-01-15T18:13:56.496575Z","iopub.status.idle":"2024-01-15T18:13:57.161279Z","shell.execute_reply.started":"2024-01-15T18:13:56.496541Z","shell.execute_reply":"2024-01-15T18:13:57.159574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming y_val and y_val_pred_f are your data arrays\n\nplt.hist(y_val, label='True Values', alpha=0.5, density=True, bins=70, color='blue', edgecolor='black')\nplt.hist(y_val_pred_f, label='Predicted Values', alpha=0.5, density=True, bins=10, color='orange', edgecolor='black')\n\nplt.title('True vs Predicted Values on Test - Label 1 ')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:13:52.618895Z","iopub.execute_input":"2024-01-15T18:13:52.620253Z","iopub.status.idle":"2024-01-15T18:13:53.226519Z","shell.execute_reply.started":"2024-01-15T18:13:52.620197Z","shell.execute_reply":"2024-01-15T18:13:53.225097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_dim = 1740\n# # autoencoder = Autoencoder(input_dim, wandb.config.bottleneck_dim)\n# # autoencoder.load_state_dict(torch.load(\"/kaggle/input/models-eda/autoencoder_v16.pth\"))\n# autoencoder = FullModel(256, 1, Encoder(input_dim, 256))\n# autoencoder.load_state_dict(torch.load(\"/kaggle/input/models-eda/predictor_v19.pth\"))\n# validate_autoencoder(autoencoder, validation_dataloader, criterion)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:12:17.853349Z","iopub.execute_input":"2024-01-15T18:12:17.853716Z","iopub.status.idle":"2024-01-15T18:12:17.859337Z","shell.execute_reply.started":"2024-01-15T18:12:17.853683Z","shell.execute_reply":"2024-01-15T18:12:17.857881Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}